#!/usr/bin/env node

/**
 * Scrape legacy content.
 *
 * Usage: scripts/importer > legacy.json
 *
 * Required: `npm install -g cheerio isomorphic-fetch`.
 */
if (process.env.NODE_PATH === undefined) {
  // http://stackoverflow.com/a/24963546/2103996
  module.paths.unshift('/usr/local/lib/node_modules');
}

const cheerio = require('cheerio'); // Tested with v0.22.0.
const crypto = require('crypto');
const fs = require('fs');
const fetch = require('isomorphic-fetch'); // Tested with v2.2.1.
const path = require('path');

const SITE = [
  {
    url: 'https://www.wincent.com/a/about/wincent/weblog/archives/',
    basename: 'blog',
  },
  {
    url: 'https://www.wincent.com/a/news/archives/',
    basename: 'blog',
  },
  {
    url: 'https://www.wincent.com/a/about/wincent/weblog/nightlog/archives/',
    basename: 'snippets',
  },
  {
    url: 'https://www.wincent.com/a/about/wincent/weblog/mini-log/archives/',
    basename: 'snippets',
  },
  {
    url: 'https://www.wincent.com/a/about/wincent/weblog/svn-log/archives/',
    skipDupes: true,
    basename: 'snippets',
  },
];

/**
 * Create a digest of some string content.
 */
function getHash(string) {
  const shasum = crypto.createHash('sha1');
  shasum.update(string);
  return shasum.digest('hex');
}

/**
 * Extract date string from input formatted like:
 *
 *    Posted by wincent at March 25, 2008  1:18 AM
 */
function extractDate(string) {
  const match = (/\b\w+\s+\d+,\s+\d{4}\s+\d+:\d+\s*[ap]m\b/i).exec(string);
  if (match) {
    return new Date(Date.parse(match[0])).toISOString();
  }
  warn(`failed to extract date from: "${string}"`);
  return null;
}

const BOLD = '\x1b[1m';
const RED = '\x1b[31m';
const RESET = '\x1b[0m';

function log(message) {
  process.stderr.write(`${message}\n`);
}

function warn(message) {
  process.stderr.write(`${BOLD}${RED}warning: ${message}${RESET}\n`);
}

function getProgress() {
  return `(active: ${active.length}, queued: ${queue.length}, done: ${doneCount}, total: ${totalCount})`;
}

function toPermalink(title) {
  return title
    .toLowerCase()
    .replace(/[?!'"]/g, '')
    .replace(/\. /g, '-')
    .replace(/\.(com|org)/g, '-$1')
    .replace(/[^\w.]/g, '-')
    .replace(/-+/g, '-')
    .replace(/[-.]+$/, '');
}

async function get(url) {
  log(`GET ${url} ${getProgress()}`);
  const response = await fetch(url);
  const text = await response.text()
  const $ = cheerio.load(text);
  return {
    url: response.url, // Reflects possible redirect.
    $,
  };
}

// For pre-existing content, we don't bother computing a hash; we always assume
// it to be unique.
const EMPTY_HASH = '0000000000000000000000000000000000000000';

function getExistingPermalinks() {
  const permalinks = new Map();
  const entries = fs.readdirSync(path.join(__dirname, '../../../content/blog'));
  entries.forEach(entry => {
    const permalink = entry.replace(/(\.\w+?)$/, '');
    permalinks.set(permalink, [EMPTY_HASH]);
  });
  return permalinks;
}

const dump = [];
const startTime = Date.now();

function postProcess(dump) {
  // Trim content.
  dump.forEach(scraped => {
    scraped.html = scraped.html.trim();
  });

  // Sort by timestamp.
  dump.sort((a, b) => {
    if (a.posted > b.posted) {
      return 1;
    } else if (a.posted < b.posted) {
      return -1;
    }
    return 0;
  });

  // Add file info.
  const permalinks = {
    blog: getExistingPermalinks(),
    snippets: new Map(),
  };
  dump.forEach(scraped => {
    let permalink = toPermalink(scraped.title);
    const {hash, type} = scraped;
    if (permalinks[type].has(permalink)) {
      // Only few cases where we'll get a real blog post clash
      // ("subversion-maintenance", because there are two legacy posts
      // with the same title, and "what-is-a-release-candidate" which
      // exists in both legacy and modern forms).
      //
      // Lots of clashes in svn-log but we'll just delete those (they are
      // literal duplicates). Others in the mini-log too but they are legit
      // content so I will name them accordingly.
      //
      // I also thought we had some cross-posts too, which we detect
      // (and omit) via a content hash. Turns out, I was wrong. They're not
      // cross-posts (eg. "/blog/keeping-up-to-date" and
      // "/blog/keeping-up-to-date-1").
      const duplicates = permalinks[type].get(permalink);
      if (duplicates.indexOf(hash) !== -1) {
        warn(`skipping cross-posted ${permalink}`);
        return;
      } else {
        const alternate = `${permalink}-${duplicates.length}`;
        warn(`permalink ${permalink} already exists; using ${alternate}`);
        duplicates.push(hash);
        permalink = alternate;
      }
    } else {
      permalinks[type].set(permalink, [hash]);
    }
    scraped.file = path.join(scraped.file, permalink);
  });

  return dump;
}

function done() {
  process.stdout.write(JSON.stringify(postProcess(dump), null, 2) + '\n');
  const endTime = Date.now();
  const delta = ((endTime - startTime) / 1000).toFixed(2);
  log(`Completed in ${delta} seconds.`);
}

const JOBS = 5;
const queue = [];
const active = [];
let doneCount = 0;
let totalCount = 0;

async function poll() {
  while (active.length < JOBS && queue.length) {
    const job = queue.shift();
    active.push(job);
    job();
  }
  if (active.length === 0) {
    done();
  }
}

function enqueue(fn) {
  totalCount++;
  queue.push(async () => {
    await fn();
    doneCount++;
    active.splice(active.indexOf(fn), 1);
    poll();
  });
  if (active.length < JOBS) {
    poll();
  }
}

SITE.forEach(site => {
  // Will append actual filename during post-processing.
  const file = path.join('content', site.basename);

  enqueue(async () => {
    const {url, $} = await get(site.url);
    const archives = $('.content li a')
      .map((i, element) => $(element).attr('href'))
      .get();

    archives.forEach(archive => {
      enqueue(async () => {
        const {url, $} = await get(archive);
        const type = site.basename;

        if (site.basename === 'blog') {
          const posts = $('.content p > a')
            .filter((i, element) => $(element).text() === 'Read full article')
            .map((i, element) => $(element).attr('href'))
            .get();

          posts.forEach(post => {
            enqueue(async () => {
              const {url, $} = await get(post);
              const $content = $('.content');
              const posted = extractDate($content.find('p.posted').text());

              // Special case: there is one of these without a title in the
              // entire corpus.
              const title = $content.find('> h3:first-of-type').text().trim() ||
                'Bugzilla 3.0 improvements';

              // Nasty song and dance to get it to actually remove the
              // unwanted nodes before we call `html()`. Mostly discovered by
              // trial and error.
              const $post = $('.content > h3:first-of-type ~');
              $post.remove('p[class]');
              const $prefix = $('.content').children().not($post);
              $('.content').children().remove($prefix);
              const html = $('.content').html();
              const hash = getHash(html);

              dump.push({
                file,
                hash,
                html,
                posted,
                title,
                type,
                url,
              });
            });
          });
        } else if (site.basename === 'snippets') {
          const $snippets = $('.content');
          const seen = new Set();
          let date;
          let title;
          let time;
          let startIndex;
          let endIndex;
          $snippets.children().each((i, child) => {
            const $child = $(child);
            if (child.tagName === 'h4') {
              date = $child.text();
            } else if (child.tagName === 'h5') {
              title = $child.text();
              startIndex = i + 1;
            } else if ($child.hasClass('posted')) {
              time = $child.text().replace(/Posted /, '');
              posted = extractDate(`${date} ${time}`);
              endIndex = i;

              // Here goes our song and dance again.
              const $clone = $snippets.clone();
              const count = $clone.children().length;
              $clone.children().remove(`:nth-last-child(-n+${count - endIndex})`);
              $clone.children().remove(`:nth-child(-n+${startIndex})`);
              $clone.children().remove('p[class]');
              const html = $clone.html();
              const hash = getHash(html);

              if (!seen.has(title) || !site.skipDupes) {
                seen.add(title);
                dump.push({
                  file,
                  hash,
                  html,
                  posted,
                  title,
                  type,
                  url,
                });
              }
            }
          });
        }
      });
    });
  });
});
